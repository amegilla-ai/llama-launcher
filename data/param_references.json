{
  "common": {
    "-h": "--hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment",
    "--version": "show version and build info",
    "-c": "--ctx-size-draft N               size of the prompt context for the draft model (default: 0, 0 = loaded",
    "--completion-bash": "print source-able bash completion script for llama.cpp",
    "--verbose-prompt": "print a verbose prompt before generation (default: false)",
    "-t": "--threads-batch-draft N          number of threads to use during batch and prompt processing (default:",
    "-C": "--cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch",
    "--cpu-strict": "use strict CPU placement (default: 0)",
    "--prio": "set process/thread priority : low(-1), normal(0), medium(1), high(2),",
    "--poll": "use polling level to wait for work (0 - no polling, default: 50)",
    "--cpu-strict-batch": "use strict CPU placement (default: same as --cpu-strict)",
    "--prio-batch": "set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime",
    "--poll-batch": "use polling to wait for work (default: same as --poll)",
    "-n": "keep the Mixture of Experts (MoE) weights of the first N layers in the",
    "-b": "--batch-size N                   logical maximum batch size (default: 2048)",
    "-u": "--ubatch-size N                  physical maximum batch size (default: 512)",
    "--keep": "number of tokens to keep from the initial prompt (default: 0, -1 =",
    "--swa-full": "use full-size SWA cache (default: false)",
    "-f": "--flash-attn [on|off|auto]       set Flash Attention use ('on', 'off', or 'auto', default: 'auto')",
    "--perf": "whether to enable internal libllama performance timings (default:",
    "-e": "--escape                         process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\) (default: true)",
    "--rope-scaling": "RoPE frequency scaling method, defaults to linear unless specified by",
    "--rope-scale": "RoPE context scaling factor, expands context by a factor of N",
    "--rope-freq-base": "RoPE base frequency, used by NTK-aware scaling (default: loaded from",
    "--rope-freq-scale": "RoPE frequency scaling factor, expands context by a factor of 1/N",
    "--yarn-orig-ctx": "YaRN: original context size of model (default: 0 = model training",
    "--yarn-ext-factor": "YaRN: extrapolation mix factor (default: -1.0, 0.0 = full",
    "--yarn-attn-factor": "YaRN: scale sqrt(t) or attention magnitude (default: -1.0)",
    "--yarn-beta-slow": "YaRN: high correction dim or alpha (default: -1.0)",
    "--yarn-beta-fast": "YaRN: low correction dim or beta (default: -1.0)",
    "-k": "use single unified KV buffer for the KV cache of all sequences",
    "--no-host": "bypass host buffer allowing extra buffers to be used",
    "-d": "use default Qwen 2.5 Coder 1.5B (note: can download weights from the",
    "--mlock": "force system to keep model in RAM rather than swapping or compressing",
    "--mmap": "whether to memory-map model (if disabled, slower load but may reduce",
    "--numa": "attempt optimizations that help on some NUMA systems",
    "--list-devices": "print list of available devices and exit",
    "-s": "--slot-prompt-similarity SIMILARITY",
    "-m": "--model-vocoder FNAME            vocoder model for audio generation (default: unused)",
    "--check-tensors": "check model tensor data for invalid values (default: false)",
    "--override-kv": "advanced option to override model metadata by key. may be specified",
    "--op-offload": "whether to offload host tensor operations to device (default: true)",
    "--lora": "path to LoRA adapter (can be repeated to use multiple adapters)",
    "--lora-scaled": "path to LoRA adapter with user defined scaling (can be repeated to use",
    "--control-vector": "add a control vector",
    "--control-vector-scaled": "add a control vector with user defined scaling SCALE",
    "--log-disable": "Log disable",
    "--log-file": "Log to file",
    "--log-colors": "Set colored logging ('on', 'off', or 'auto', default: 'auto')",
    "-v": "--verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for",
    "--offline": "Offline mode: forces use of cache, prevents network access",
    "-l": "--logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,",
    "--log-prefix": "Enable prefix in log messages",
    "--log-timestamps": "Enable timestamps in log messages",
    "--samplers": "samplers that will be used for generation in the order, separated by",
    "--ignore-eos": "ignore end of stream token and continue generating (implies",
    "--temp": "temperature (default: 0.8)",
    "--top-k": "top-k sampling (default: 40, 0 = disabled)",
    "--top-p": "top-p sampling (default: 0.9, 1.0 = disabled)",
    "--min-p": "min-p sampling (default: 0.1, 0.0 = disabled)",
    "--top-nsigma": "top-n-sigma sampling (default: -1.0, -1.0 = disabled)",
    "--xtc-probability": "xtc probability (default: 0.0, 0.0 = disabled)",
    "--xtc-threshold": "xtc threshold (default: 0.1, 1.0 = disabled)",
    "--typical": "locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)",
    "--repeat-last-n": "last n tokens to consider for penalize (default: 64, 0 = disabled, -1",
    "--repeat-penalty": "penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)",
    "--presence-penalty": "repeat alpha presence penalty (default: 0.0, 0.0 = disabled)",
    "--frequency-penalty": "repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)",
    "--dry-multiplier": "set DRY sampling multiplier (default: 0.0, 0.0 = disabled)",
    "--dry-base": "set DRY sampling base value (default: 1.75)",
    "--dry-allowed-length": "set allowed length for DRY sampling (default: 2)",
    "--dry-penalty-last-n": "set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 =",
    "--dry-sequence-breaker": "add sequence breaker for DRY sampling, clearing out default breakers",
    "--dynatemp-range": "dynamic temperature range (default: 0.0, 0.0 = disabled)",
    "--dynatemp-exp": "dynamic temperature exponent (default: 1.0)",
    "--mirostat": "use Mirostat sampling.",
    "--mirostat-lr": "Mirostat learning rate, parameter eta (default: 0.1)",
    "--mirostat-ent": "Mirostat target entropy, parameter tau (default: 5.0)",
    "--grammar": "BNF-like grammar to constrain generations (see samples in grammars/",
    "--grammar-file": "file to read grammar from",
    "-j": "--json-schema-file FILE          File containing a JSON schema to constrain generations",
    "--no-perf": "disable internal libllama performance timings (default: false)",
    "--no-escape": "do not process escape sequences",
    "--no-mmap": "do not memory-map model (slower load but may reduce pageouts if not",
    "--no-op-offload": "disable offloading host tensor operations to device (default: false)",
    "--context-shift": "whether to use context shift on infinite text generation (default:",
    "-r": "--reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode",
    "--spm-infill": "use Suffix/Prefix/Middle pattern for infill (instead of",
    "--pooling": "pooling type for embeddings, use model default if unspecified",
    "--image-min-tokens": "minimum number of tokens each image can take, only used by vision",
    "--image-max-tokens": "maximum number of tokens each image can take, only used by vision",
    "-a": "--alias STRING                   set alias for model name (to be used by REST API)",
    "--host": "ip address to listen, or bind to an UNIX socket if the address ends",
    "--port": "port to listen (default: 8080)",
    "--path": "path to serve static files from (default: )",
    "--api-prefix": "prefix path the server serves from, without the trailing slash",
    "--embedding": "restrict to only support embedding use case; use only with dedicated",
    "--rerank": "enable reranking endpoint on server (default: disabled)",
    "--api-key": "API key to use for authentication (default: none)",
    "--api-key-file": "path to file containing API keys (default: none)",
    "--ssl-key-file": "path to file a PEM-encoded SSL private key",
    "--ssl-cert-file": "path to file a PEM-encoded SSL certificate",
    "--chat-template-kwargs": "sets additional params for the json template parser",
    "--threads-http": "number of threads used to process HTTP requests (default: -1)",
    "--cache-reuse": "min chunk size to attempt reusing from the cache via KV shifting",
    "--metrics": "enable prometheus compatible metrics endpoint (default: disabled)",
    "--props": "enable changing global properties via POST /props (default: disabled)",
    "--slots": "expose slots monitoring endpoint (default: enabled)",
    "--slot-save-path": "path to save slot kv cache (default: disabled)",
    "--jinja": "whether to use jinja template engine for chat (default: enabled)",
    "--reasoning-format": "controls whether thought tags are allowed and/or extracted from the",
    "--reasoning-budget": "controls the amount of thinking allowed; currently only one of: -1 for",
    "--chat-template": "set custom jinja chat template (default: template taken from model's",
    "--lora-init-without-apply": "load LoRA adapters without applying them (apply later via POST",
    "--draft": "number of tokens to draft for speculative decoding (default: 16)",
    "--draft-min": "minimum number of draft tokens to use for speculative decoding",
    "--draft-p-min": "minimum speculative decoding probability (greedy) (default: 0.8)",
    "--spec-replace": "translate the string in TARGET into DRAFT if the draft model and main",
    "--tts-use-guide-tokens": "Use guide tokens to improve TTS word recall",
    "--embd-gemma-default": "use default EmbeddingGemma model (note: can download weights from the",
    "--fim-qwen-3b-default": "use default Qwen 2.5 Coder 3B (note: can download weights from the",
    "--fim-qwen-7b-default": "use default Qwen 2.5 Coder 7B (note: can download weights from the",
    "--fim-qwen-7b-spec": "use Qwen 2.5 Coder 7B + 0.5B draft for speculative decoding (note: can",
    "--fim-qwen-14b-spec": "use Qwen 2.5 Coder 14B + 0.5B draft for speculative decoding (note:",
    "--fim-qwen-30b-default": "use default Qwen 3 Coder 30B A3B Instruct (note: can download weights",
    "--gpt-oss-20b-default": "use gpt-oss-20b (note: can download weights from the internet)",
    "--gpt-oss-120b-default": "use gpt-oss-120b (note: can download weights from the internet)",
    "--vision-gemma-4b-default": "use Gemma 3 4B QAT (note: can download weights from the internet)",
    "--vision-gemma-12b-default": "use Gemma 3 12B QAT (note: can download weights from the internet)"
  },
  "server": {
    "-k": "use single unified KV buffer shared across all sequences (default:",
    "--warmup": "whether to perform warmup with an empty run (default: enabled)",
    "--mmproj-offload": "whether to enable GPU offloading for multimodal projector (default:",
    "--webui": "whether to enable the Web UI (default: enabled)",
    "--media-path": "directory for loading local media files; files can be accessed via",
    "--models-dir": "directory containing models for the router server (default: disabled)",
    "--models-preset": "path to INI file containing model presets for the router server",
    "--models-max": "for router server, maximum number of models to load simultaneously"
  },
  "cli": {
    "--no-context-shift": "disables context shift on infinite text generation (default: enabled)",
    "--no-warmup": "skip warming up the model with an empty run",
    "--mmproj": "path to a multimodal projector file. see tools/mtmd/README.md",
    "--mmproj-url": "URL to a multimodal projector file. see tools/mtmd/README.md",
    "--no-mmproj": "explicitly disable multimodal projector, useful when using -hf",
    "--no-mmproj-offload": "do not offload multimodal projector to GPU",
    "--no-webui": "Disable the Web UI (default: enabled)",
    "--no-slots": "disables slots monitoring endpoint",
    "--no-prefill-assistant": "whether to prefill the assistant's response if the last message is an"
  }
}