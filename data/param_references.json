{
  "common": {
    "-m": "Path to the main model file (.gguf). Long form: --model.",
    "-c": "Context window size in tokens. 0 uses model default. Long form: --ctx-size.",
    "-t": "Number of CPU threads used during generation. Long form: --threads.",
    "--device": "Comma-separated devices used for offloading model layers.",
    "--main-gpu": "Primary GPU index when not using tensor splitting.",
    "--split-mode": "Defines how the model is split across GPUs (none, layer, row).",
    "--tensor-split": "Fractional split of model tensors across GPUs.",
    "-b": "Logical maximum batch size for generation. Long form: --batch-size.",
    "--ubatch-size": "Physical upper limit for batch processing; affects memory usage.",
    "--cache-type-k": "Data type used for the Key (K) portion of the KV cache.",
    "--cache-type-v": "Data type used for the Value (V) portion of the KV cache.",
    "--cache-reuse": "Minimum chunk size required to reuse KV cache via shifting.",
    "--no-host": "Allows extra host buffers instead of strictly device-local buffers.",
    "--temp": "Sampling temperature controlling randomness.",
    "--top-p": "Nucleus sampling cumulative probability threshold.",
    "--top-k": "Limits sampling to the top K most likely tokens.",
    "--min-p": "Filters out tokens below a minimum probability.",
    "-s": "Random seed for reproducible generation. Long form: --seed.",
    "--ignore-eos": "Ignores EOS tokens and continues generation.",
    "--numa": "Enables NUMA-aware optimizations.",
    "--cpu-mask": "CPU affinity mask controlling which cores are used.",
    "--cpu-range": "CPU core range used for thread affinity.",
    "--cpu-strict": "Enforces strict CPU placement.",
    "--rope-scaling": "RoPE scaling method used to extend context length.",
    "--rope-scale": "Multiplicative factor applied to RoPE context.",
    "--rope-freq-scale": "Frequency scaling factor for NTK-aware RoPE.",
    "--yarn-orig-ctx": "Original training context size for YaRN.",
    "--yarn-ext-factor": "YaRN extrapolation factor.",
    "--model-draft": "Draft model used for speculative decoding to accelerate generation.",
    "--draft-p-min": "Minimum probability to accept draft predictions.",
    "--device-draft": "Devices used to offload the draft model.",
    "--lora": "Path to a LoRA adapter.",
    "--lora-scaled": "LoRA adapter applied with scaling factor.",
    "--lora-init-without-apply": "Loads LoRA adapters without applying them.",
    "--grammar": "Inline BNF-style grammar constraining output.",
    "--grammar-file": "Grammar file used to constrain generation.",
    "--json-schema": "Inline JSON schema restricting output.",
    "--json-schema-file": "JSON schema file for structured generation.",
    "--mirostat": "Enables Mirostat adaptive sampling.",
    "--mirostat-ent": "Target entropy for Mirostat.",
    "--mirostat-lr": "Learning rate for Mirostat.",
    "--dry-base": "Base value for DRY sampling.",
    "--dry-multiplier": "Multiplier applied to DRY sampling.",
    "--dry-penalty-last-n": "Token window used for DRY penalties.",
    "--chat-template": "Selects chat template, usually from model metadata.",
    "--chat-template-file": "External Jinja chat template file.",
    "--chat-template-kwargs": "Extra parameters passed to chat template.",
    "--mmproj": "Path to multimodal projection file required for vision models.",
    "--image-max-tokens": "Maximum token budget per image input.",
    "--version": "Prints build and version info then exits.",
    "--list-devices": "Lists available compute devices and exits.",
    "--cache-list": "Lists cached models.",
    "--completion-bash": "Outputs a bash completion script."
  },
  "server": {
    "--port": "TCP port the HTTP server listens on.",
    "--host": "Address or UNIX socket the server binds to.",
    "--parallel": "Maximum number of concurrent inference slots.",
    "--threads-http": "Number of worker threads handling HTTP requests.",
    "--timeout": "HTTP read/write timeout in seconds.",

    "--api-key": "Static API key required for authenticated requests.",
    "--api-key-file": "File containing one or more API keys.",
    "--api-prefix": "Base path prefix for all REST API endpoints.",

    "--models-dir": "Directory containing models for router mode.",
    "--models-max": "Maximum number of models loaded at once.",
    "--models-preset": "INI file defining router presets.",

    "--metrics": "Enables Prometheus-compatible metrics endpoint.",
    "--slots": "Enables monitoring endpoint for inference slots.",
    "--slot-save-path": "Persist slot KV cache to disk.",
    "--slot-prompt-similarity": "Similarity threshold for slot reuse.",

    "--fit": "Automatically adjusts parameters to fit memory.",
    "--fit-ctx": "Minimum context size when auto-fitting.",
    "--fit-target": "Memory margin target per device.",

    "--ssl-cert-file": "PEM-encoded SSL certificate for HTTPS.",
    "--ssl-key-file": "PEM-encoded SSL private key for HTTPS.",

    "--log-file": "Writes logs to the specified file.",
    "--log-prefix": "Prefix added to all log messages.",
    "--log-timestamps": "Prepends timestamps to log messages.",
    "--log-colors": "Enables colored logging output.",
    "--log-disable": "Disables all logging."
  },
  "cli": {
    "-i": "Enables interactive chat mode. Long form: --interactive.",
    "--reverse-prompt": "Stops generation when a token sequence is encountered.",
    "--context-shift": "Enables infinite generation by shifting context.",
    "--no-context-shift": "Disables context shifting.",

    "--cont-batching": "Enables continuous (dynamic) batching.",
    "--no-cont-batching": "Disables continuous batching.",

    "--no-mmap": "Disables memory-mapped model loading.",
    "--no-kv-offload": "Prevents KV cache from being offloaded to GPU.",
    "--no-op-offload": "Disables offloading of host tensor operations.",

    "--escape": "Processes escape sequences like \\n and \\t.",
    "--no-escape": "Disables escape sequence processing.",

    "--no-warmup": "Skips initial warmup inference run.",
    "--no-perf": "Disables internal performance timing.",

    "--verbose-prompt": "Prints expanded prompt before generation.",
    "--jinja": "Enables Jinja rendering for chat output.",
    "--no-prefill-assistant": "Disables assistant response prefill.",
    "--no-webui": "Disables embedded Web UI."
  }
}
